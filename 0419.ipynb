{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767d44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def simclr_loss(z_i: torch.Tensor, z_j: torch.Tensor, temperature: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the simCLR loss function for a batch of paired feature vectors.\n",
    "\n",
    "    Args:\n",
    "        z_i (torch.Tensor): A tensor of shape (N, D) representing the feature vectors of the first views.\n",
    "        z_j (torch.Tensor): A tensor of shape (N, D) representing the feature vectors of the second views.\n",
    "        temperature (float): The temperature parameter for the softmax operation. Default: 0.5.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A scalar tensor representing the simCLR loss for the given batch of paired feature vectors.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If z_i and z_j do not have the same shape or if the shape of z_i or z_j is not (N, D).\n",
    "    \"\"\"\n",
    "    # Normalize the feature vectors\n",
    "    z_i = F.normalize(z_i, dim=1)\n",
    "    z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "    # Concatenate the feature vectors and create the targets\n",
    "    z = torch.cat([z_i, z_j], dim=0)\n",
    "    targets = torch.arange(z.size(0)).to(z.device)\n",
    "    masks = F.one_hot(targets, num_classes=z.size(0))\n",
    "\n",
    "    # Compute the similarities between all pairs of feature vectors\n",
    "    similarities = torch.matmul(z, z.t()) / temperature\n",
    "\n",
    "    # Set the diagonal elements (i.e., the similarities between each feature vector and itself) to negative infinity\n",
    "    mask = masks.float().neg()\n",
    "    similarities = similarities.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "    # Compute the numerator and denominator of the loss function\n",
    "    numerator = torch.exp(similarities)\n",
    "    denominator = numerator.sum(dim=1, keepdim=True)\n",
    "\n",
    "    # Compute the loss function\n",
    "    loss = -torch.log(numerator / denominator).mean()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import torch\n",
    "from simclr_loss import simclr_loss\n",
    "\n",
    "class TestSimCLRLoss(unittest.TestCase):\n",
    "    def test_simclr_loss(self):\n",
    "        # Set the random seed for reproducibility\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        # Create a batch of feature vectors\n",
    "        batch_size = 32\n",
    "        embedding_dim = 128\n",
    "        z_i = torch.randn(batch_size, embedding_dim)\n",
    "        z_j = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "        # Compute the simCLR loss\n",
    "        loss = simclr_loss(z_i, z_j)\n",
    "\n",
    "        # Check that the loss is a scalar tensor\n",
    "        self.assertIsInstance(loss, torch.Tensor)\n",
    "        self.assertEqual(loss.dim(), 0)\n",
    "\n",
    "        # Check that the loss is non-negative\n",
    "        self.assertGreaterEqual(loss.item(), 0)\n",
    "\n",
    "        # Check that the loss is finite\n",
    "        self.assertTrue(torch.isfinite(loss).all())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
